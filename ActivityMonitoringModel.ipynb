{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da58040d",
   "metadata": {},
   "source": [
    "# Activity Monitoring (Machine Learning Project)\n",
    "### &nbsp; &nbsp; BSEF21M001 - Yeshal Khan\n",
    "### &nbsp; &nbsp; BSEF21M008 - Zohaib Shahid\n",
    "### &nbsp; &nbsp; BSEF21M016 - Faiqa Nasir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49757878",
   "metadata": {},
   "source": [
    "# Importing the necessary Libraries for the activity monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7a5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba376d4a",
   "metadata": {},
   "source": [
    "# Path to the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3878fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:\\\\5th Semester\\\\ML\\\\ML Project\\\\bbh\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44f9ed",
   "metadata": {},
   "source": [
    "# Loading the training and testing data and label files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd64edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "    # Load training data and training labels from files\n",
    "    train_accel_ms = np.load(data_path + \"training/trainMSAccelerometer.npy\")\n",
    "    train_gyro_ms = np.load(data_path + \"training/trainMSGyroscope.npy\")\n",
    "    train_accel = np.load(data_path + \"training/trainAccelerometer.npy\")\n",
    "    train_gravity = np.load(data_path + \"training/trainGravity.npy\")\n",
    "    train_accel_jin = np.load(data_path + \"training/trainJinsAccelerometer.npy\")\n",
    "    train_gyro_jin = np.load(data_path + \"training/trainJinsGyroscope.npy\")\n",
    "    train_lin_accel = np.load(data_path + \"training/trainLinearAcceleration.npy\")\n",
    "    train_magnetometer = np.load(data_path + \"training/trainMagnetometer.npy\")\n",
    "    train_gyro = np.load(data_path + \"training/trainGyroscope.npy\")\n",
    "    train_labels = np.load(data_path + \"training/trainlabels.npy\")\n",
    "\n",
    "    # Load testing data and testing labels from files\n",
    "    test_accel_ms = np.load(data_path + \"testing/testMSAccelerometer.npy\")\n",
    "    test_gyro_ms = np.load(data_path + \"testing/testMSGyroscope.npy\")\n",
    "    test_accel = np.load(data_path + \"testing/testAccelerometer.npy\")\n",
    "    test_gravity = np.load(data_path + \"testing/testGravity.npy\")\n",
    "    test_accel_jin = np.load(data_path + \"testing/testJinsAccelerometer.npy\")\n",
    "    test_gyro_jin = np.load(data_path + \"testing/testJinsGyroscope.npy\")\n",
    "    test_lin_accel = np.load(data_path + \"testing/testLinearAcceleration.npy\")\n",
    "    test_magnetometer = np.load(data_path + \"testing/testMagnetometer.npy\")\n",
    "    test_gyro = np.load(data_path + \"testing/testGyroscope.npy\")\n",
    "    test_labels = np.load(data_path + \"testing/testlabels.npy\")\n",
    "\n",
    "    training_data = [train_accel_ms, train_gyro_ms, train_accel, train_gravity, train_accel_jin, train_gyro_jin, train_lin_accel, train_magnetometer, train_gyro]\n",
    "    testing_data = [test_accel_ms, test_gyro_ms, test_accel, test_gravity, test_accel_jin, test_gyro_jin, test_lin_accel, test_magnetometer, test_gyro]\n",
    "\n",
    "    return training_data, train_labels, testing_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "641f0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, train_labels, testing_data, test_labels = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9eccff",
   "metadata": {},
   "source": [
    "# Applying Pre-Processing Techniques on each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6eb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the training and testing data using the StandardScaler\n",
    "def pre_process_dataset_scalar(data_sets):\n",
    "    pre_processed_data = []\n",
    "    for data_set in data_sets:\n",
    "        scalar = StandardScaler()\n",
    "        impute = SimpleImputer()\n",
    "        N,T,S=data_set.shape\n",
    "        reshaped_data = np.reshape(data_set, (N, T*S))\n",
    "        imputed_data = impute.fit_transform(reshaped_data)\n",
    "        pre_processed_data.append(scalar.fit_transform(imputed_data).reshape((N, T, S)))\n",
    "    return pre_processed_data\n",
    "\n",
    "# Pre-process the training and testing data using the Normalizer\n",
    "def pre_process_dataset_normalizer(data_sets):\n",
    "    pre_processed_data = []\n",
    "    for data_set in data_sets:\n",
    "        normalizer = Normalizer()\n",
    "        impute = SimpleImputer()\n",
    "        N,T,S=data_set.shape\n",
    "        reshaped_data = np.reshape(data_set, (N, T*S))\n",
    "        imputed_data = impute.fit_transform(reshaped_data)\n",
    "        pre_processed_data.append(normalizer.fit_transform(imputed_data).reshape((N, T, S)))\n",
    "    return pre_processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2946d",
   "metadata": {},
   "source": [
    "# Feature Extraction for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02029b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    features = []\n",
    "    for data_set in data:\n",
    "        features.append(np.mean(data_set, axis=1))\n",
    "        features.append(np.max(data_set, axis=1))\n",
    "        features.append(np.min(data_set, axis=1))\n",
    "        features.append(np.std(data_set, axis=1))\n",
    "        features.append(np.var(data_set, axis=1))\n",
    "        features.append(np.median(data_set, axis=1))\n",
    "        features.append(np.percentile(data_set, 25, axis=1))\n",
    "        features.append(np.percentile(data_set, 75, axis=1))\n",
    "    return np.concatenate(features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80a8a4",
   "metadata": {},
   "source": [
    "# Dimensions of Testing and Training data after the pre-processing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f47120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after pre-processing and feature extraction:  (2284, 216)\n",
      "Testing data shape after pre-processing and feature extraction:  (2288, 216)\n"
     ]
    }
   ],
   "source": [
    "X_train_scalar = extract_features(pre_process_dataset_scalar(training_data))\n",
    "X_test_scalar = extract_features(pre_process_dataset_scalar(testing_data))\n",
    "\n",
    "X_train_normalizer = extract_features(pre_process_dataset_normalizer(training_data))\n",
    "X_test_normalizer = extract_features(pre_process_dataset_normalizer(testing_data))\n",
    "\n",
    "print(\"Training data shape after pre-processing and feature extraction: \", X_train_scalar.shape)\n",
    "print(\"Testing data shape after pre-processing and feature extraction: \", X_test_scalar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184261fd",
   "metadata": {},
   "source": [
    "# Training and Testing of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a45f3",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a661cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Evaluation Metrics\n",
      "===========================================\n",
      "Accuracy:  68.23 %\n",
      "F1 Score:  0.68\n",
      "Recall Score:  0.68\n",
      "Confusion Matrix:\n",
      "[[39  0  0 ...  0  0  0]\n",
      " [ 0 51  0 ...  0  0  0]\n",
      " [ 0 10 20 ...  0  1  1]\n",
      " ...\n",
      " [ 0  0  3 ... 17  0  0]\n",
      " [ 0  0  0 ...  0 39  0]\n",
      " [ 0  0  0 ...  0  0 33]]\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train_normalizer, train_labels)\n",
    "predictions = random_forest.predict(X_test_normalizer)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions, average='macro')\n",
    "recall = recall_score(test_labels, predictions, average='macro')\n",
    "rf_confusion = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "print(\"Random Forest Classifier Evaluation Metrics\")\n",
    "print(\"===========================================\")\n",
    "print(f\"Accuracy:  {accuracy * 100:.2f} %\")\n",
    "print(f\"F1 Score:  {f1:.2f}\")\n",
    "print(f\"Recall Score:  {recall:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(rf_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124e8a7",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5cefd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Evaluation Metrics\n",
      "===========================================\n",
      "Accuracy:  52.23 %\n",
      "F1 Score:  0.68\n",
      "Recall Score:  0.52\n",
      "Confusion Matrix:\n",
      "[[38  0  0 ...  0  0  0]\n",
      " [ 0 46  4 ...  0  0  0]\n",
      " [ 0 10 13 ...  2  0  0]\n",
      " ...\n",
      " [ 0  1  4 ... 12  0  0]\n",
      " [ 0  0  0 ...  0 34  0]\n",
      " [ 0  0  0 ...  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "SVM = SVC()\n",
    "SVM.fit(X_train_normalizer, train_labels)\n",
    "predicted_labels = SVM.predict(X_test_normalizer)\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "recall = recall_score(test_labels, predicted_labels, average='macro')\n",
    "averageF1 = f1_score(test_labels, predicted_labels, average='macro')\n",
    "SVM_confusion_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "print(\"SVM Classifier Evaluation Metrics\")\n",
    "print(\"===========================================\")\n",
    "print(f\"Accuracy:  {accuracy * 100:.2f} %\")\n",
    "print(f\"F1 Score:  {f1:.2f}\")\n",
    "print(f\"Recall Score:  {recall:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(SVM_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab602e",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a3077c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier Evaluation Metrics\n",
      "=================================================\n",
      "Accuracy:  57.30 %\n",
      "F1 Score:  0.57\n",
      "Recall Score:  0.58\n",
      "Confusion Matrix:\n",
      "[[38  0  0 ...  0  0  0]\n",
      " [ 0 44  4 ...  0  0  0]\n",
      " [ 0  2 21 ...  1  0  0]\n",
      " ...\n",
      " [ 0  1  1 ...  8  0  0]\n",
      " [ 0  0  0 ...  0 22  0]\n",
      " [ 0  0  0 ...  1  0 25]]\n"
     ]
    }
   ],
   "source": [
    "Logistic_Regression = LogisticRegression(max_iter=3000)\n",
    "Logistic_Regression.fit(X_train_scalar,train_labels)\n",
    "predicted_labels = Logistic_Regression.predict(X_test_scalar)\n",
    "accuracy = accuracy_score(test_labels,predicted_labels)\n",
    "recall = recall_score(test_labels,predicted_labels,average='macro')\n",
    "averageF1 = f1_score(test_labels,predicted_labels,average='macro')\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "recall = recall_score(test_labels, predicted_labels, average='macro')\n",
    "averageF1 = f1_score(test_labels, predicted_labels, average='macro')\n",
    "logistic_Regression_confusion_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "print(\"Logistic Regression Classifier Evaluation Metrics\")\n",
    "print(\"=================================================\")\n",
    "print(f\"Accuracy:  {accuracy * 100:.2f} %\")\n",
    "print(f\"F1 Score:  {averageF1:.2f}\")\n",
    "print(f\"Recall Score:  {recall:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(logistic_Regression_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23a388",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146ebffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Evaluation Metrics\n",
      "===========================================\n",
      "Accuracy:  43.27 %\n",
      "F1 Score:  0.41\n",
      "Recall Score:  0.44\n",
      "Confusion Matrix:\n",
      "[[39  0  0 ...  0  0  0]\n",
      " [ 0 43  1 ...  0  1  0]\n",
      " [ 0  8 12 ...  0  3  0]\n",
      " ...\n",
      " [ 0  0  1 ...  1  0  1]\n",
      " [ 0  0  1 ...  0 31  0]\n",
      " [ 0  0  0 ...  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "Naive_Bayes = GaussianNB()\n",
    "Naive_Bayes.fit(X_train_scalar, train_labels)\n",
    "predicted_labels = Naive_Bayes.predict(X_test_scalar)\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "recall = recall_score(test_labels, predicted_labels, average='macro')\n",
    "averageF1 = f1_score(test_labels, predicted_labels, average='macro')\n",
    "naive_bayes_confusion_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "print(\"Naive Bayes Classifier Evaluation Metrics\")\n",
    "print(\"===========================================\")\n",
    "print(f\"Accuracy:  {accuracy * 100:.2f} %\")\n",
    "print(f\"F1 Score:  {averageF1:.2f}\")\n",
    "print(f\"Recall Score:  {recall:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(naive_bayes_confusion_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
